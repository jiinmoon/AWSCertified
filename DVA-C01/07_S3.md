# Simple Storage Service

- One of most important section!
- 'Infinitely scaling' storage.

## S3 Buckets

- S3 allows people to store objects (files) in buckets (directories).
- Buckets require **globally unique name**.
- Buckets are defined at the region level.

## S3 Objects

- Objects (files) have a key.
- **Key** is the full path:
    - i.e. `s3://my-bucket/my_file.txt` my_file.txt is the key.
    - i.e. `s3://my-bucket/my_folder/my_file.txt` my_folder/my_file.txt.

- Key is composed of prefix and object name.
    - i.e. `this_folder/that_folder/my_file.txt`.
    - `this_folder/that_folder/` is the prefix.
    - `my_file.txt` is the object name.

- There is no concept of 'directories' within buckets.
    - It visually tricks you, but all it is that key:value storage.
    - In short, it is just a key name containing "/".

- Object values are content of the boddy:
    - Max object size is 5 TB.
    - Over 5 GB, you must use multi-part upload.

- Metadata (list of text key : value pairs - system or user metadata).

- Tags
- Version ID

## S3 Versioning

- You can version your files in S3.
- Enabled at the bucket level.
- Same key overwrite will increment the version: 1, 2, 3...
- Best practice is to version your buckets:
    - Protect against uninteded deletes (to restore a version).
    - Easy roll back to previous version.

- Any file that is not versioned prior to enabling versioning will have verion
    'null'.
- Suspending versioning odes not delete the previous versions.

## S3 Encryption for Objects

- Four methods for encrypting objects in S3.
    - SSE-S3 : encrypts S3 objects using keys managed by AWS.
    - SSE-KMS : uses AWS Key Management Service.
    - SSE-C : supply your own encryption keys.
    - Client Side Encryption : encrypt your self.

- Important to know for exam; which situations requires what encryption method?

### SSE-S3

- Amazon S3 manages the key.
- Object is encrypted in server side.
- AES-256 encryption type.
- Set the header: `x-amz-server-side-encryption":"AES256"`.
- i.e. when we upload the object via HTTP/S to S3, we set the header to include
    above encryption denotation; then, S3 will encrypted the object to place in
    the bucket.

### SSE-KMS

- KMS gives advantage of user control + audit trail.
- Set the header : `x-amz-server-side-encryption":"awsLkms"`.
- The uploaded object will be encrypted server side using KMS Customer Master
    Key.

### SSE-C

- You supply key to encrypt server-side.
- S3 does not manage the encryption key you provide.
- HTTPS is required since we are sending the secret key.
- The encryption key is required for each HTTP request made, in HTTP headers.

### Client Side Encryption

- Client lib such as the Amazon S3 Encryption Client helps.
- Client is responsible for encryption/decryption.
- Customer is in full charge of key and encryption cycle.

## Encryption in Trainsit (SSL/TLS)

- Amazon S3 exposes:
    - HTTP endpoint
    - HTTPS endpoint : encryption in flight.

- Free to use which one to upload; HTTPS is recommended.

---

## S3 Security

- User based:
    - IAM policies - which API calls should be allowed for a specific user from
        IAM console.

- Reousrce based:
    - Bucket Policies - bucket wide rules from S3 console (cross account).
    - Object Access Control List (ACL) - finer control.
    - Bucket Access Control List (ACL) - less common.

- IAM principal can access an S3 object if
    - the user IAM permissions allow it OR the resource policy ALLOWS it.
    - AND there should not be DENY.

### Bucket Policies

- Can use AWS Policy Generator.

- JSON based policies.
    - Resources: Buckets and Objects.
    - Actions: Set of API to Aloow or Deny.
    - Effect: Allow or Deny.
    - Principal: The account or user to apply the policy to.

- Use S3 bucket policy to:
    - grant public access to the bucket.
    - force objects to be encrypted at upload.
        - i.e. DenyUnencryptedObjects...
    - grant access to another account (cross account).

### Bucket settings for Block Public Access

- Block public access to buckets and objects granted through
    - new ACLs
    - any ACLs
    - new public bucket or access point policies.

- Block public and cross-account access to buckets and objects through any
    public bucket or access point policies.

- Theses settings were created to prevent company data leaks.
- If your bucket should never be public, set this on.
- Can be set at the account level.

## S3 Security -- other

- Networking:
    - Supports VPC Endpoints (for intances in VPC without www internet).

- Logging and Audit:
    - S3 Access Logs can be stored in other S3 bucket.
    - API calls can be logged in AWS CloudTrail.

- User Security:
    - MFA Delete : Multi-factor Authentication can be required in versioned
        buckets to delete objects!
    - Pre-signed URLs : URLs that are valid for only for a limited time (ex:
        premium video service for logged in users).

---

## S3 Websites

- S3 can host static websites and access from www.
- The URL would be:
    - `<bucket-name>.s3-website-<AWS-region>.amazonaws.com`.

- Do not forget to set your bucket policy to allow public read.
    - Effect : ALLOW
    - Principal : `*`
    - Service : S3
    - Action : GetObject

---

## CORS

- **Origin** is a scheme (protocol), host (domain), and port.
    - i.e. https://www.example.com
        - HTTPS port 443; HTTP port 80.

- **Cross-Origin Resource Sharing**
- Web Browser based mechanism to allow requests to other origins while visiting
    the main origin.

- **Same Origin** has a same host.
    - i.e. http://example.com/app1 and http://example.com/app2 has same origin.

- Request wont be fullfilled unless the othe rorigin allows for the requests,
    using CORS Headers (`Access-Control-Allow-Origin`).

- Suppose we have a web browser. There is a web server which is our origin,
    https://www.example.com. And cross origin https://www.other.com exists as
    well.
- Web browser first access Origin, and will need to make a pre-flight request to
    Cross Origin. Cross Origin will response back, allowing browser to make
    requests freely.

### S3 CORS

- If a client does a cross-origin request on our S3 bucket, we need to enable
    the correct CORS headers.
- Popular exam question; when and where to enable CORS.
- You can allow for a specific origin or for `*` (all).

- Web browser will perform GET request to bucket-html (enabled as a website),
    and bucket will return the file (index.html for example).
- However, it will also require browser to make a subsequent CORS request to
    GET additional files (say, images as linked in index.html) to another CORS 
    enabled bucket-assets to grab image resources.
- So long as the bucket-assets is configured correctly with header
    Access-Control-Allow-Origin:
    bucket-html.s3-website.us-west-1.amazonaws.com...

- Remember, CORS has to be enabled by `other origin`, not the actual origin that
    it is referring the browser to.

- `CORS_CONFIG.xml` should be used; it's example is as follows:

    ```xml
    <CORSConfiguration>
        <CORSRule>
            <AllowedOrigin>origin-bucket-url</AllowedOrigin>
            <AllowedMethod>GET</AllowedMethod>
            <MaxAgeSeconds>3000</MaxAgeSecond>
            <AllowedHeader>Authorization</AllowedHeader>
        </CORSRule>
    </CORSConfiguration>
    ```

---

## S3 Consistency Model

- **PUT** : Read after write consistency.
    - Soon as object is written, you can retrieve it.
        - PUT 200 -> GET 200.
    - However, if we did GET prior to PUT before object existed, we can get 404.
        - GET 404 -> PUT 200 -> GET 404
        - Eventually consistent.

- **DELETES** and **PUTS** : Eventual Consistency.
    - If we read an object after updating, we may get older version.
        - PUT 200 -> PUT 200 -> GET 200 that is first PUT.
    - If we delete an object, we might still be able to retrieve it for a short
        time!
        - DELETE 200 -> GET 200.

- There isn't a way to request for strong consistency.


