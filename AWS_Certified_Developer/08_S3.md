# Simple Storage Service (S3)

- S3 provides devs with secure, druable, highly-scalable object storage.
- S3 is easy to use, with a simple web services interface to store and retrieve
    any amount of data from anywhere onthe web.

- Key here is that it is for object storage; pictures, media, audios... This is
    not for boot volumes nor databases.

- The data is spread across multiple devices and facilities.

## S3 is...

- Object based; mainly for uploading files.
- Files can be from 0 bytes to 5 TB.
- Unlimited storage.
- Files are stored in Buckets (like folder).
- S3 has a universal namespace; meaning that names must be unique globally (like
    hostname).
    - i.e. https://s3-na-west-1.amazonaws.com/YOURBUCCKETNAME

- When file is uploaded to S3 successfully, you will receieve HTTP 200 code.

## Data Consistency Model For S3

- **Read after Write Consistency** for PUTS of new Objects.
    - Uploaded file is available as soon as it finishes uploading.
- **Eventual Consistency** for overwrite PUTS and DELETES.
    - Updating a file takes some time to propagate.
 
## S3 is a simple key-value store!

- S3 is object based where objects consists of the following:
    - Key (name of object).
    - Value (the data; sequence of bytes).
    - Version ID (important for versioning).
    - Metadata (data about data stored).
    - Subresource - bucket-specific configuration:
        - Bucket Policies, Access Control Lists... 
        - Cross Origin Resource Sharing (CORS).
        - Transfer Acceleration (service that speeds up uploading).

## S3 is...

- built in mind for 99.99% availability for the S3 platform.
- Amazon actually gurantees 99.9% availability.
- Amazon also gurantees 99.999999999% durability for S3 information (11 x 9 s).
    - durability : amount of data expected to be lost.
    - while this is really good, always make backups/versioning/so on.
- Tiered Storage Available.
- Lifecycle Management.
- Versioning.
- Encryption.
- Secure Access points to your data via Access Control Lists/Bucket Policies.

## S3 Storage Tiers/Classes

- S3
    - 99.99% availability, 99.999999999% durability, stored redundantly across
        multiple devices in multiple facilities, and is designed to sustain the
        loss of 2 facilities concurrently.

- S3 - IA (Infrequently Accessed)
    - For data that is accessed less frequently, but requires rapid access when
        needed. Lower fee than S3, but charged a retrieval fee.

- S3 - One Zone IA
    - Same as IA, but data is stored ina single AZ only; still 99.999999999%
        durability, but only 99.5% availability. Cost is 20% less than regular
        IA.

- Reduced Redundancy Storage
    - 99.99% durability and 99.99% availiability of objects over a given year.
    - Used for data that can be easily reprocuded if lost (i.e. thumbnails).

- Glacier
    - Very cheap; archival purposes only.
    - It can take hours 3 - 5 to retrieve from Glacier.

## S3 Intelligent Tiering?

- additional class; used for unknown or unpredictable access patterns.
- 2 tiers : frequent and infrequent access.
- automatically moves your data to most cost-effective tier based on how
    frequently you acccess each object.
- has same availiabilty and durability as the S3; 99.999999999% dur/99.9 avail.
- optimizes cost.
- no fees for accessing your data but a small monthly fee for
    monitoring/automation $0.0025 per 1000 objects.

## What do you get charged for with S3?

- Storage per GB
- Requests (Get, Put, Copy...)
- Storage Management Pricing
    - Inventory, Analytics, and Object Tags
- Data management Pricing
    - Data transferred out of S3 (download from S3)
- Transfer Acceleration
    - use CloudFront to optimize transfers

---

# Exam Tips

- S3 is **object based**; should not be used to run OS or databases.
- Files can be 0 to 5 TB.
- Unlimited Storage.
- uploaded files are stored in Buckets.
- S3 has universal namesapce -- names should be unique globally.

- Two Consistency Models:
    - read after write for new PUTS.
    - eventual consistency for overwrite PUTS and DELETE.
        - takes time to propagate.

- Storage Classes:
    - S3 (standard)
        - durable, immediately available, frequently accessed.
    - S3 - IA
        - infreqeuntly accessed.
    - S3 - One Zone IA
        - data is stored in only single availability zone.
    - S3 - Reduced Redundancy Storage
        - for easily reproducible data.
    - Glacier 
        - archived data.

- S3 Objects have:
    - Key (object name)
    - Value (data storing)
    - Version ID
    - Metadata
    - Subresources - bucket-specific configuration:
        - Bucket Policies, Access Control Lists
        - Cross Origin Resource Sharing (CORS)
        - Transfer Acceleration

- Successful upload generate 200 HTTP status code when using CLI or API.

- read up on the aws.amazon.com/s3/faqs

---

# S3 Security

## Securing Your Buckets

- By default, all newly created buckets are PRIVATE.
    - Meaning that only the owner of the bucket gets access to the bucket.
    - We can change this so that other users may access the bucket as well.

- Owner can set up access control to your buckets using:
    - Bucket Policies 
        - Applied at a bucket level.
        - Written in JSON (there is a tool to generate easily).
    - Access Control Lists - Applied at a object level.

- S3 buckets can be configured to create access logs, which log all request made
    to the S3 ucket. Theses logs can be written to another bucket.

## S3 ACLs & Policies follow-along

- Here, we will change a bucket's policies and access control lists.

- Log in to the management console and go to S3 bucket.
- Create a new bucket if you do not have one yet.
    - When creating a new bucket, it gives few options:
        - Versioning : maintains the versions of objects within the bucket;
            meaning that if an object is deleted by a mistake, you can still
            revert back.
        - Server access logging : maintains a record of access logs to bucket.
        - Enable Encryption (AES-256 or ...).
    - Then, we set permissions as to who can access the bucket.
        - Block public access is turned on by default.
    - But in this, we select defaults.
- Once done, wait a while until bucket is provisioned.
- Go into your bucket and let's create a new directionry and try to upload any
    file.
    - When we upload the file, we can first change the access control per object
        basis.
        - By default, the owner (uploader) has read/write permissions.
        - And we can add additional AWS account for access, or
        - Set the object for public read access.
    - Then, we select storage class:
        - Standard; Intelligent Tiering; IA; One Zone IA; Glacier...
    - Encryption method.
    - Metadata.
    - Tags.
- We can immediately view the uploaded data if we would like.
- The uploaded object also has a https URL link to it.
    - But most likely you will receive an access denied error.
    - This is due to disabling public access by default.
    - When trying this on browser, the browser does not make the request with
        the AWS credentials; so of course, it won't access.
- We can make it publicly available by changing configuration when we are
    uploading or selecting the object, and make it public.
- We can also attach policy to the bucket by using policy generator.
    - Principal is who we are applying to.
    - ARN is the name of service we are applying to. So, in this case, bucket
        name.
    - When generated, it gives JSON formatted policy; which can be copy and
        pasted to the Bucket Policy editor and saved.

---

# S3 Encryption

## Two types of encryption

- In Transit:
    - encrypting the file while transferring (from your pc to bucket or bucket
        to bucket...).
    - SSL/TLS (HTTPS)

- At Rest:
    - Server-Side Encryption:
        - S3 Managed Keys, **SSE-S3**.
        - AWS Key Management Service, Managed Keys, **SSE-KMS**.
            - additional features such as added level of encryption.
            - encrypting the encryption key.
            - logs the encryption/decryption events.
        - Server Side ENcryption with Customer Provided Keys, **SSE-C**
    - Client-Side Encryption
        - You encrypt it yourself!

## Enforcing Encryption on S3 Buckets

- How do we set the bucket such that we automatically encrypt all uploaded
    files?

- Everytime a file is uploaded to S3, a PUT request is initiated.

    ```
    PUT /myFile HTTP/1.1
    Host: myBucket.s3.amazonaws.com
    Date: Mon, 11 Jan 2020 01:11:00 GMT
    Authorization: authorization string
    Content-Type: text/plain
    Contect-Length: 27364
    x-amz-meta-author: jmoon
    Expect: 100-continue
    [27364 bytes of object data]
    ```

- If a file is to be encrypted at **upload time**, the
    **x-amz-server-side-encryption parameter** will be included in the request
    header.
- Two Encryption options are available:
    - **x-amz-server-side-encryption: AES256** (SSE-S3, S3 managed keys)
    - **x-amz-server-side-encryption: ams:kms** (SSE-KMS, KMS managed keys)
- When this paramter is included in the header of the PUT request, it tells S3
    to encrypt the object at the time of upload, using the specified encryption
    method.

- You can enforce the use of Server Side Encryption by using a Bucket Policy
    which denies any S3 PUT request which doesn't include the
    **x-amz-server-side-encryption** parameter in the request header.

---

# S3 Encryption Exam Tips

- Encryption In-Transit: SSL/TLS (HTTPS)
- Encryption At-Rest:
    - Server Side Encryption:
        - SSE-S3
        - SSE-KMS
        - SSE-C
    - Client Side Encryption
- To enforce use of encryption for your files stored in S3, use S3 Bucket Policy
    to deny all PUT requests that do not include
    **x-amz-server-side-encryption** parameter in the request header.

---

# S3 Encryption on An S3 Bucket follow-along

- Let's try creating a bucket to add in the encryption bucket policy; and test
    by uploading the file.

- Let's log in to AWS management console and goto S3 services.

- Create a new bucket;
- Easiest way to enable Encryption would be to check the Default encryption
    option when we are creating the new bucket.
    - We can select between AES-256 (SSE-S3) or KMS keys (SSE-KMS).
- But for this follow-along, let's use Bucket Policy to enforce the encryption
    instead.
- Now, go into the Permissions tab of created bucket and see Bucket Policy.
- Let's genereate the new Bucket Policy.
    - Effect will be **Deny**.
    - Principal will be all, wildcard.
    - Actions would be PutObject.
    - And we need to add the condition;
        - Condition: StringNotEquals
        - Key : s3: x-amz-server-side-encryption
        - Value : aws:kms
- We can now apply the generated policy.
- But notice that we have an error message, 'Action does not apply to any
    resources in statement'.
    - this is a common issue; resolve by editing the Resource field, adding wild
        card in the end 'arn:aws:s3:::bucket-name/wildcard'.
- Now let's try to upload an object.
- When we try to upload an object without encryption, you will see that upload
    will be failed. Further examining it will specify that it is Forbidden.
- Now, upload an object with encryption; should be accepted if encryption was
    SSE-KMS.

---

# CORS Configuration follow-along

- We will enable Cross Object Region Sharing.
- Allows one code within the S3 bucket to reference another code in another S3
    bucket.

- Log-in to AWS management console and goto S3 services.
- Create a default bucket, but allowing public access.
- Go into the bucket created, and select Properties tab.
- Here, we will select Static website hosting.
    - When we do so, it will give us Endpoint URL; which is used to access the
        website.
- Now, let's upload few files onto the bucket.
    -  index.html
    
    ```html
    <script src="https://code.jquery.com/jquery-1.11.0.min.js"></script>
    <script src="https://code.jquery.com/jquery-migrate-1.2.1.min.js"></script>
    <html>
        <body>
            <h1>This is the Index Page!</h1>

            <div id="get-html-from-other-s3"></div>
            <script>
            $("#get-html-from-other-s3").load("loadpage.html")
            </script>
    ```

    - loadpage.html

    ```html
    <h2>I am from another bucket!</h2>
    </body>
    </html>
    ```
- Now, we should be able to get our static web going by visiting the endpoint
    URL. The web should load contents from index.html, and index.html should run
    javascript to add contents from loadpage.html.

- Then, we are going to see whether we can loadpage from another bucket. Delete
    the loadpage from your bucket; then, create another bucket.
- On newly created bucket, enable static website hosting; and upload
    loadpage.html.
- Visit newly created bucket's endpoint URL/loadpage.html to confirm it is
    accessible.

- Now, on the original index.html, make changes to load the 'loadpage.html' from
    the new 'loadpage.html' at the new bucket.
- Upload the modified index.html to the original bucket, and then try to access
    it now.
- We now shouldn't be able to see the second line which comes from the
    loadpage.html in another bucket; which is strange as another bucket is also
    publicly accessible.
    - To further understand the error, enable developer console to see
        javascript error message. It should read, "Cross-Origin Request Blocked:
        THe Same Origin Policy disallows reading the remote resource at
        http://anotherbucket.region.amazonaws.com/loadpage.html (Reason: CORS
        header 'Acess-Control-Aollow-Origin' missing)."
    - To fix this, we will now enable CORS.

- In order to set CORS, we need URL for our index.ht
- In order to set CORS, we need URL for our index.html, a file that is trying to
    access another. Copy Endpoint static website.
- Go to anothe bucket where loadpage.html is; under Permissions, we should see
    CORS configuration.
    - COR configuration is done via JSON.

        ```
        <CORSConfiguration>
            <CORSRule>
                <AllowedOrigin>*</AllowedOrigin>
                <AllowedMethod><GET</AllowedMethod>
                <MaxAgeSeconds>3000</MaxAgeSeconds>
                <AllowedHeader>Authorization</AllowedHeader>
            </CORSRule>
        </CORSConfiguration>
        ```

    - Change the wildcard within AllowedOrigin to our endpoint/index.html.
    - Save the above into the editor.

- Now, it should work!

---

# CloudFront

- Amazon's CDN - Cloud Delivery Network.



